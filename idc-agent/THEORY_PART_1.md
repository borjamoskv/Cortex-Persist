# La Ciencia del Agente: Hacia una Unificación de Teoría de la Información, Decisión Bayesiana y Control Cibernético

## 1. El Triángulo Fundacional de la Ciencia del Agente

La construcción de agentes inteligentes genuinamente autónomos ha estado históricamente fragmentada entre múltiples disciplinas: la teoría de la información provee herramientas para cuantificar la incertidumbre, la estadística bayesiana ofrece mecanismos para actualizar creencias, y la ingeniería de control desarrolla sistemas para regular el comportamiento dinámico. Sin embargo, ninguna de estas aproximaciones por sí sola ha logrado producir agentes que exhiban las propiedades integrales de la inteligencia natural: adaptación continua, robustez ante perturbaciones, y aprendizaje eficiente de la experiencia. La **Ciencia del Agente** emerge como propuesta unificadora que reconoce la interdependencia fundamental de tres pilares teóricos, cada uno desempeñando un rol distintivo pero inseparable en la arquitectura de un sistema inteligente completo.

La **Teoría de la Información** constituye la base epistemológica, proporcionando el lenguaje matemático para medir qué sabe y qué ignora el agente. La **Decisión Bayesiana** funciona como el motor computacional, transformando información en acciones mediante la actualización coherente de creencias y la optimización de decisiones bajo incertidumbre. El **Control Cibernético** opera como estabilizador dinámico, garantizando que el comportamiento del agente permanezca acotado, robusto y homeostático frente a perturbaciones del entorno. Juntos, estos tres elementos forman un triángulo conceptual donde cada vértice refuerza y condiciona los otros dos, creando una síntesis que trasciende la mera yuxtaposición de técnicas.

### 1.1 Teoría de la Información como Base

#### 1.1.1 Fundamentos conceptuales: entropía, información mutua y cuantificación de la incertidumbre

La **entropía de Shannon**, definida como **H(X) = −Σ p(x) log p(x)**, constituye la medida cardinal de incertidumbre en un sistema probabilístico. Para un agente, la entropía de sus creencias sobre el estado del mundo representa directamente su grado de ignorancia: valores elevados indican que múltiples hipótesis son igualmente plausibles, mientras que valores bajos señalan certeza concentrada en pocas posibilidades. Esta cuantificación no es meramente descriptiva, sino operacionalmente crucial, ya que permite al agente calcular explícitamente cuánto "le falta por aprender" y dirigir sus recursos hacia la reducción sistemática de esta incertidumbre .

La **información mutua** **I(X;Y) = H(X) − H(X|Y) = H(Y) − H(Y|X)** mide la reducción de incertidumbre sobre una variable que se obtiene al conocer otra. En el contexto del aprendizaje por refuerzo, esta cantidad adquiere un rol central al cuantificar exactamente cuánto aprende un agente sobre el entorno al ejecutar una acción particular y observar su resultado. El análisis riguroso de **Thompson Sampling**, uno de los algoritmos más efectivos para el dilema exploración-explotación, demuestra cómo la información mutua entre la acción óptima (desconocida) y la observación resultante permite establecer límites superiores teóricos sobre el **regret bayesiano** —la pérdida acumulada por no conocer la mejor acción desde el inicio .

El **Information Ratio (IR)**, introducido por Russo y Van Roy, encapsula esta conexión de manera elegante: **IR_t = E[regret_t]² / I(a*; (a_t, o_t) | H_t)**, donde el numerador cuantifica el costo inmediato de la incertidumbre y el denominador cuantifica la información ganada. Políticas que mantienen este ratio acotado garantizan un crecimiento sublineal del regret, es decir, **aprendizaje eficiente en el sentido de que el costo promedio de la ignorancia tiende a cero** .

| Concepto | Definición | Rol en el agente |
|----------|-----------|------------------|
| Entropía H(X) | −Σ p(x) log p(x) | Mide ignorancia del agente sobre el mundo |
| Información mutua I(X;Y) | H(X) − H(X\|Y) | Cuantifica valor informativo de observaciones |
| Divergencia KL D_KL(P\|\|Q) | Σ p(x) log(p(x)/q(x)) | Mide costo de aproximar P con modelo Q |
| Information Ratio | E[regret]² / I(a*; observación) | Balancea costo de incertidumbre vs. aprendizaje |

La **divergencia de Kullback-Leibler** **D_KL(P||Q) = Σ p(x) log(p(x)/q(x))** cuantifica la "distancia" asimétrica entre dos distribuciones de probabilidad, siendo fundamental para evaluar la calidad de las aproximaciones del agente y para derivar principios de actualización de creencias. La no-negatividad de la divergencia KL —**D_KL(P||Q) ≥ 0 con igualdad si y solo si P = Q**— proporciona una garantía matemática de que las aproximaciones siempre incurrirán en alguna pérdida de información, salvo que sean perfectas. Esta propiedad subyace a la **descomposición de la energía libre** en términos de precisión y complejidad, que se discutirá en la sección de marcos unificadores.

#### 1.1.2 Aplicación en representación de estados y flujo de información sensorial

La representación interna del estado del mundo por parte de un agente puede entenderse como un proceso de **codificación informacional con restricciones de recursos**. Los estados sensoriales brutos —señales de receptores en sistemas biológicos, vectores de características en sistemas artificiales— deben transformarse en representaciones comprimidas que preserven la información relevante para la supervivencia o el logro de objetivos. Este proceso de compresión con pérdida está fundamentalmente limitado por el **principio de rate-distortion** de la teoría de la información: dado un presupuesto de bits para la representación interna, ¿cuál es la codificación que minimiza el error esperado en las decisiones subsiguientes?

En el contexto del aprendizaje por refuerzo, esta perspectiva ha llevado al desarrollo de métodos de **aprendizaje de representaciones basados en información**. El principio de **Predictive Information Maximization** postula que las representaciones óptimas son aquellas que maximizan la información mutua entre el estado actual y el futuro predecible, descartando simultáneamente el ruido irrelevante. Esto conecta profundamente con la noción de **modelos generativos** en neurociencia computacional, donde el cerebro se concibe como un sistema que mantiene representaciones internas capaces de generar predicciones sensoriales precisas .

El flujo de información a través de las jerarquías de procesamiento también puede analizarse mediante la teoría de la información. La **bottleneck information** —la información mutua entre capas sucesivas— revela cómo el sistema descarta progresivamente información irrelevante mientras preserva invariantes útiles para la tarea. Análisis recientes de redes neuronales entrenadas con backpropagation exhiben automáticamente una **fase inicial de "ajuste"** donde la información sobre los datos de entrada crece, seguida de una **fase de "compresión"** donde esta información se reduce mientras mejora la generalización. Este fenómeno, predicho por la teoría de la información de rate-distortion, explica empíricamente por qué las redes profundas generalizan a pesar de su enorme capacidad de sobreajuste.

Para agentes con **restricciones de canal de comunicación**, la teoría de la información impone límites fundamentales sobre el desempeño alcanzable. La capacidad de canal **C = max_p(x) I(X;Y)** determina la tasa máxima de información confiable que puede transmitirse, lo cual tiene implicaciones directas para el diseño de sistemas sensoriales en robótica y para la compresión de información en agentes con recursos computacionales limitados. Un robot con ancho de banda de comunicación restringido entre sensores y unidad de procesamiento debe comprimir selectivamente la información sensorial, reteniendo solo aquella que es crítica para el control .

#### 1.1.3 Principio de máxima entropía y su rol en modelado de agentes

El **principio de máxima entropía (MaxEnt)**, desarrollado por Jaynes, establece que dada información parcial en forma de valores esperados de ciertas funciones, la distribución de probabilidad que mejor representa el estado de conocimiento actual —en el sentido de hacer las menores suposiciones adicionales— es aquella que **maximiza la entropía sujeta a esas restricciones**. Este principio tiene aplicaciones directas en el diseño de **priors no informativos** y en la **regularización de políticas de aprendizaje por refuerzo**.

En el contexto de la toma de decisiones, el principio de máxima entropía ha sido utilizado para derivar algoritmos de **"soft optimality"** donde la política óptima no es determinista sino estocástica, con una temperatura que controla el trade-off entre recompensa esperada y entropía de la política. Esta formulación, conocida como **MaxEnt RL** o **maximum entropy reinforcement learning**, produce comportamientos más robustos y exploratorios que las políticas deterministas tradicionales. La entropía de la política actúa como una regularización que previene la sobreconfianza prematura y facilita la exploración continua, especialmente en entornos con múltiples modos de comportamiento óptimo o parcialmente observables .

La conexión con la termodinámica es profunda y no meramente analógica. La función de valor en MaxEnt RL puede interpretarse como una **energía libre**, donde la recompensa acumulada corresponde a la energía interna y la entropía de la política a la entropía termodinámica. Esta correspondencia ha permitido importar técnicas de mecánica estadística —métodos de Monte Carlo con cadenas de Markov, algoritmos de recocido simulado— al diseño de algoritmos de aprendizaje por refuerzo eficientes. Para un agente que opera con conocimiento incompleto, el principio de MaxEnt ofrece una **prescripción normativa para la representación de incertidumbre**: asignar máxima entropía consistente con la información disponible evita sesgos indeseables que podrían comprometer el aprendizaje posterior.

#### 1.1.4 Conexión con la teoría algorítmica de la información (AIXI, complejidad de Kolmogorov)

La **teoría algorítmica de la información**, desarrollada por Kolmogorov, Chaitin y Solomonoff, extiende la teoría de Shannon cuantificando la **información intrínseca de objetos individuales** en lugar de distribuciones de probabilidad. La **complejidad de Kolmogorov K(x)** de una cadena x es la longitud del programa más corto que produce x en una máquina universal de Turing. Esta noción tiene implicaciones profundas para la teoría del aprendizaje: el **principio de inducción de Solomonoff** establece que la predicción óptima dada datos observados se obtiene ponderando todas las hipótesis computables por su probabilidad a priori, donde esta probabilidad decrece exponencialmente con la complejidad de Kolmogorov de la hipótesis.

**AIXI**, desarrollado por Marcus Hutter, representa la formalización más ambiciosa de estas ideas en el contexto de agentes inteligentes. AIXI es un **agente teóricamente óptimo para cualquier entorno computable**, definido como aquel que maximiza la recompensa esperada ponderada sobre todas las posibles computaciones consistentes con la historia observada. La definición involucra explícitamente la suma sobre todos los programas de una máquina universal, ponderados por **2^{−longitud(programa)}**, es decir, por su probabilidad algorítmica a priori. Aunque AIXI es incomputable en la práctica debido a la imposibilidad de calcular la complejidad de Kolmogorov, proporciona un **"faro teórico"** que ilumina las propiedades deseables de cualquier agente de aprendizaje: **optimalidad bayesiana, exploración automática, y convergencia a comportamiento óptimo en el límite** .

| Aspecto | AIXI teórico | Aproximaciones prácticas |
|---------|-----------|------------------------|
| Optimalidad | Universal, asintótica | Limitada por recursos computacionales |
| Exploración | Automática, ponderada por complejidad | Heurísticas de exploración explícitas |
| Modelo del mundo | Suma sobre todos los programas | Familias paramétricas restringidas |
| Actualización | Bayesiana exacta | Inferencia variacional, MCMC, etc. |
| Control | Implícito en maximización de recompensa | MPC, policy gradients, etc. |

La relevancia de AIXI para la Ciencia del Agente propuesta es que demuestra formalmente que los **tres pilares —información, bayes y control— no son elecciones de diseño arbitrarias sino consecuencias de una definición de inteligencia**. La base informacional aparece en la ponderación por complejidad; el motor bayesiano en la actualización de creencias sobre entornos; y el control en la maximización de recompensa esperada. Cualquier aproximación práctica que aspire a capturar estas propiedades debe necesariamente incorporar versiones computables de estos tres componentes.

### 1.2 Decisión Bayesiana como Motor

#### 1.2.1 Inferencia bayesiana para actualización de creencias en tiempo real

La **inferencia bayesiana** proporciona el mecanismo fundamental por el cual un agente transforma sus creencias ante nueva evidencia. El **teorema de Bayes**, **P(H|D) ∝ P(D|H) P(H)**, describe cómo las creencias previas sobre hipótesis deben actualizarse ante nueva evidencia para producir creencias posteriores. El prior **P(H)** codifica el conocimiento previo del agente, la verosimilitud **P(D|H)** especifica qué evidencia se espera bajo cada hipótesis, y el posterior **P(H|D)** representa el conocimiento actualizado. La proporcionalidad indica que el aprendizaje es **multiplicativo en el espacio de probabilidades**: cada nueva observación modula las creencias previas de manera que el resultado es siempre una distribución de probabilidad válida.

La implementación práctica de esta actualización en tiempo real presenta desafíos computacionales significativos. Para modelos complejos, el posterior no tiene forma cerrada y debe aproximarse. Los **métodos de Monte Carlo con cadenas de Markov (MCMC)** proporcionan muestras asintóticamente exactas pero son computacionalmente costosos. La **inferencia variacional** aproxima el posterior con una familia paramétrica más simple, optimizando los parámetros para minimizar una divergencia (típicamente la divergencia KL inversa). Los **filtros de partículas** mantienen una representación de muestras ponderadas que se actualiza secuencialmente, siendo particularmente adecuados para espacios de estado no lineales y no gaussianos.

En el contexto de agentes físicos, la **velocidad de la inferencia es crítica**. Un vehículo autónomo que debe tomar decisiones en milisegundos no puede permitirse ciclos de MCMC completos. Esto ha motivado el desarrollo de **arquitecturas de "inferencia amortizada"**, donde una red neuronal es entrenada para producir aproximaciones al posterior dado datos, reduciendo la inferencia a una evaluación de red feedforward. Estas técnicas, conocidas como **VAEs (Variational Autoencoders)** en el contexto generativo o **redes de inferencia** en el contexto de POMDPs, representan un compromiso entre exactitud bayesiana y eficiencia computacional .

La investigación documentada en  demuestra la efectividad de la integración de inferencia bayesiana con control de aprendizaje por refuerzo en línea. El algoritmo **Bayesian SARSA** utiliza procesos gaussianos para representar funciones de valor y actualizar creencias de manera eficiente. En la tarea del **"mountain car"**, Bayesian SARSA logra encontrar la meta dentro de **5 episodios** y aprender una política casi óptima en **50 episodios**, mientras que el algoritmo tabular SARSA requiere aproximadamente **80 episodios** para encontrar la región objetivo y converge después de **150 episodios**. Esta **eficiencia muestral superior**, que reduce el número de episodios necesarios en un factor de 3 o más, es precisamente lo que hace de la inferencia bayesiana un **"motor" efectivo** para el aprendizaje del agente.

#### 1.2.2 Teorema de Bayes como mecanismo de aprendizaje inductivo

El teorema de Bayes puede interpretarse como la **formalización matemática del aprendizaje inductivo**: dada una hipótesis previa y nueva evidencia, produce una hipótesis actualizada que balancea la información histórica con la nueva. Esta interpretación, defendida por Jaynes en su obra *Probability Theory: The Logic of Science*, establece que **la inferencia bayesiana es la extensión única consistente de la lógica deductiva a situaciones donde la información es incompleta**. Para un agente de IA, esto significa que el teorema de Bayes no es simplemente una herramienta estadística entre otras, sino **el mecanismo fundamental de razonamiento bajo incertidumbre**.

El aprendizaje bayesiano exhibe propiedades de **convergencia elegantes** bajo condiciones generales. Si el prior asigna probabilidad no nula al verdadero modelo, el posterior converge casi seguramente a concentrarse en este modelo a medida que crece la evidencia. La tasa de convergencia está relacionada con la **información de Fisher**, cuantificando cuántas observaciones se requieren típicamente para distinguir modelos similares. Para agentes, esto implica que la **eficiencia muestral del aprendizaje** —cuántas interacciones con el entorno se necesitan para aprender— puede analizarse y optimizarse desde una perspectiva informacional.

Un aspecto particularmente relevante es el **manejo de priors informativos derivados de estructura**. En muchos dominios, el conocimiento previo no es arbitrario sino que refleja **regularidades físicas o estructurales del mundo**. Por ejemplo, en visión, la invariancia traslacional y la jerarquía de características son priors fuertes que pueden incorporarse en arquitecturas de red. El enfoque bayesiano proporciona un marco para formalizar estos priors como distribuciones sobre funciones o parámetros, permitiendo que el aprendizaje se beneficie de la estructura sin sacrificar la capacidad de adaptarse cuando esta estructura se viola.

#### 1.2.3 Maximización de utilidad esperada bajo incertidumbre

La **teoría de la decisión bayesiana** prescribe que, dado un estado de creencias **P(θ)** y un conjunto de acciones posibles con utilidades **U(a, θ)**, la acción óptima es aquella que **maximiza la utilidad esperada**: **a* = argmax_a E_θ[U(a, θ)]**. Este principio, aparentemente simple, tiene implicaciones profundas para el diseño de agentes. Primero, **unifica percepción y acción**: la "mejor" percepción no es aquella que es más precisa en algún sentido abstracto, sino aquella que **reduce la incertidumbre de manera relevante para las decisiones a tomar**. Segundo, proporciona una **medida de valor de la información**: cuánto aumentaría la utilidad esperada si se conociera el valor exacto de alguna variable incierta.

El cálculo de la utilidad esperada requiere **integración sobre el espacio de posibles estados del mundo**, lo cual es computacionalmente desafiante para espacios de alta dimensionalidad. Las aproximaciones Monte Carlo, donde se muestrean escenarios del posterior y se promedian sus utilidades, son el método estándar. Métodos más sofisticados explotan la estructura del problema: si la utilidad es una función simple de predicciones del modelo, técnicas de **propagación de incertidumbre analítica** pueden ser aplicables. En aprendizaje por refuerzo profundo, la función de valor **Q(s,a)** puede interpretarse como una estimación de utilidad esperada, y su entrenamiento mediante minimización de error temporal-diferencia como una forma de **aproximación estocástica de la utilidad esperada verdadera**.

La investigación en  ilustra la aplicación práctica de estos principios en sistemas de ingeniería complejos. La **optimización bayesiana** se utiliza para encontrar configuraciones óptimas de motores aeronáuticos, logrando mejoras significativas: **1501.27 N de empuje adicional** en modo de empuje máximo, **reducción del 0.38% en consumo de combustible**, y **reducción de 7.9 K en temperatura de turbina. La comparación con métodos alternativos es reveladora: la optimización bayesiana requiere solo **11+0 muestras** en modo de empuje máximo, comparado con **4927 muestras** para algoritmos genéticos y **3000** para optimización por enjambre de partículas . Esta **eficiencia muestral**, que representa reducciones de dos a tres órdenes de magnitud, es consecuencia directa de la integración apropiada de inferencia bayesiana con criterios de utilidad.

#### 1.2.4 Muestreo de Thompson y equilibrio exploración-explotación

El **muestreo de Thompson (Thompson Sampling, TS)** es quizás el algoritmo más elegante y práctico para el dilema fundamental de **exploración-explotación** en toma de decisiones secuencial. En su formulación básica, TS mantiene un posterior sobre los parámetros del entorno, muestrea un valor de estos parámetros en cada paso, y ejecuta la acción óptima para ese valor muestreado. Este procedimiento, sorprendentemente simple, logra un **balance automático entre exploración y explotación**: acciones inciertas pero potencialmente valiosas son frecuentemente muestreadas como óptimas (exploración), mientras que acciones conocidamente buenas son consistentemente seleccionadas cuando el posterior está concentrado (explotación).

El análisis teórico de TS ha revelado propiedades de **optimalidad fuertes**. En el contexto de bandits multi-brazo, TS alcanza **límites inferiores de regret información-teóricos**: no existe ningún algoritmo que pueda garantizar un crecimiento asintóticamente más lento del regret en el peor caso. Más recientemente, el análisis mediante teoría de la información ha mostrado que el desempeño de TS puede caracterizarse en términos de cuánta información adquiere sobre la acción óptima en cada interacción. Específicamente, el **regret bayesiano está acotado por una cantidad proporcional a la raíz cuadrada de la entropía del prior sobre la acción óptima**, con una constante que depende de la estructura del problema .

| Algoritmo | Regret asintótico | Exploración | Requiere ajuste de parámetros |
|-----------|-------------------|-------------|------------------------------|
| ε-greedy | O(T) | Aleatoria fija | Sí (ε) |
| UCB | O(√(T log T)) | Basada en cota de confianza | Sí (c) |
| Thompson Sampling | O(√(T log T)) | Automática, adaptativa | No |
| Information-Directed Sampling | O(√(T)) | Dirigida por información | No |

La extensión de TS a entornos con estados (MDPs) y a modelos con estructura paramétrica ha sido un área activa de investigación. En el caso lineal, donde la recompensa esperada de cada acción es una función lineal de features conocidas con pesos desconocidos, TS con prior gaussiano admite implementaciones eficientes donde el posterior se actualiza analíticamente. Para modelos no lineales y redes neuronales, aproximaciones como el **"Bayesian Neural Network Thompson Sampling"** mantienen distribuciones aproximadas sobre los pesos de la red, permitiendo la **exploración estructurada en espacios de alta dimensionalidad**. La conexión con el control cibernético emerge cuando consideramos que TS puede verse como un **controlador que ajusta su política basándose en el estado de información actual**, con la aleatoriedad del muestreo actuando como un mecanismo de **dithering** que asegura persistencia de excitación.

### 1.3 Control Cibernético como Estabilizador

#### 1.3.1 Lazos de retroalimentación negativa y homeostasis

El **Control Cibernético**, en la tradición iniciada por Norbert Wiener, proporciona los principios mediante los cuales un agente mantiene la **estabilidad y robustez** de su comportamiento ante perturbaciones y cambios del entorno. Si la Teoría de la Información cuantifica qué sabe el agente y la Decisión Bayesiana determina qué hace, el Control Cibernético **garantiza que lo que hace sea consistente, predecible y adaptable**. La cibernética, definida como **"la ciencia del control y la comunicación en animales y máquinas"**, es inherentemente interdisciplinaria y se ocupa de los **principios universales de regulación y retroalimentación**.

El concepto de **retroalimentación negativa** es el núcleo de la teoría de control. Un sistema con retroalimentación negativa compara su salida actual con un valor deseado (**setpoint**) y ajusta sus acciones para reducir la discrepancia. Este mecanismo, aparentemente simple, es **extraordinariamente poderoso**: permite que sistemas con componentes imperfectos exhiban comportamiento preciso y robusto, **compensando automáticamente perturbaciones no modeladas**. Para un agente, los setpoints pueden ser objetivos explícitos (mantener temperatura, seguir trayectoria) o implícitos (minimizar predicción de error, mantener homeostasis fisiológica).

La **homeostasis**, concepto biológico formalizado por Cannon y adoptado por la cibernética, se refiere al **mantenimiento de variables internas dentro de rangos viables**. En agentes artificiales, esto se traduce en la necesidad de mantener no solo el logro de objetivos externos sino también la **integridad operacional**: niveles de energía, temperatura de computación, integridad de datos, etc. Más profundamente, la homeostasis puede entenderse en términos de la **"Free Energy Principle"** de Friston, donde todo sistema que persiste en entornos fluctuantes debe necesariamente **minimizar la sorpresa (surprisal) de sus observaciones**, lo cual es matemáticamente equivalente a mantener un modelo generativo que prediga con precisión sus inputs sensoriales .

Los lazos de retroalimentación en agentes inteligentes operan en **múltiples escalas temporales**. A la escala más rápida, controladores de bajo nivel regulan dinámicas mecánicas o eléctricas con frecuencias de kilohertz. A escalas intermedias, controladores de comportamiento ajustan secuencias de acción basándose en resultados observados. A escalas lentas, mecanismos metacognitivos evalúan y modifican las estrategias de control mismas. Esta **jerarquía de lazos anidados**, cada uno con su propia dinámica y objetivos, es característica de sistemas cibernéticos complejos y esencial para la adaptación a perturbaciones que operan en diferentes bandas de frecuencia.

#### 1.3.2 Principios de control óptimo y teoría de control moderna

La **teoría de control óptimo**, desarrollada por Pontryagin, Bellman y otros, proporciona **métodos sistemáticos para diseñar controladores que minimizan criterios de desempeño explícitos**. El **principio del máximo de Pontryagin** caracteriza las trayectorias óptimas en términos de un sistema hamiltoniano extendido. La **programación dinámica de Bellman**, con su ecuación fundamental **V(s) = max_a [R(s,a) + γ E[V(s')]]**, proporciona una condición necesaria y suficiente para optimalidad en problemas de decisión secuencial. Estas herramientas, desarrolladas originalmente para ingeniería aeroespacial y procesos químicos, son directamente aplicables al control de agentes inteligentes.

La **teoría de control moderna**, con su énfasis en **espacio de estados** y **observabilidad/controlabilidad**, proporciona un lenguaje unificado para analizar sistemas multivariables. La descomposición de sistemas en modos observables y controlables revela **qué aspectos del entorno pueden ser influenciados por el agente y qué aspectos pueden ser conocidos a través de observación**. Para agentes parcialmente observables, el **teorema de separación** establece condiciones bajo las cuales el problema puede descomponerse en **estimación de estado óptima seguida de control óptimo dado el estado estimado**, aunque esta separación exacta raramente es alcanzable en práctica no lineal.

El **control predictivo de modelo (MPC, Model Predictive Control)** representa un paradigma particularmente relevante para agentes con modelos aprendidos. En MPC, el agente **optimiza una secuencia de acciones futuras dado su modelo actual del entorno, ejecuta solo la primera acción de esta secuencia, observa el resultado, y repite el proceso**. Este enfoque de **horizonte deslizante** proporciona **robustez ante errores de modelo y perturbaciones no anticipadas**, ya que el plan se recalcula continuamente. La integración con aprendizaje de modelos —donde el modelo utilizado en MPC se aprende y mejora con la experiencia— es un área activa que conecta directamente con los pilares de información y bayes de nuestra propuesta.

#### 1.3.3 Estabilidad mediante funciones de Lyapunov

La **teoría de estabilidad de Lyapunov** proporciona herramientas para **garantizar que un sistema permanecerá cerca de un punto de equilibrio deseado**, sin necesidad de resolver explícitamente las ecuaciones de movimiento. Una **función de Lyapunov V(x)** es una función escalar definida en el espacio de estados que es **positiva definida** (cero solo en el equilibrio, positiva en otros lugares) y cuya **derivada temporal a lo largo de trayectorias del sistema es negativa semi-definida**. La existencia de tal función garantiza **estabilidad asintótica**: el sistema convergerá al equilibrio desde cualquier condición inicial en la región donde V está definida.

Para agentes de aprendizaje, las funciones de Lyapunov tienen aplicaciones en **múltiples niveles**. A nivel de control de bajo nivel, garantizan que el seguimiento de trayectorias sea estable. A nivel de algoritmo de aprendizaje, pueden diseñarse para **garantizar convergencia**: por ejemplo, en ciertos algoritmos de gradiente estocástico, la distancia al óptimo puede servir como función de Lyapunov que decrece en expectativa. A nivel de sistema completo, la **"energía libre" en la Free Energy Principle puede interpretarse como una función de Lyapunov** para el sistema agente-entorno acoplado, garantizando que el conjunto evolucione hacia estados de mínima sorpresa mutua.

El diseño de **controladores con garantías de estabilidad certificadas** es un área de investigación activa, particularmente relevante para aplicaciones de seguridad crítica. Técnicas de **"control barrier functions"** extienden las ideas de Lyapunov para garantizar no solo estabilidad sino también **satisfacción de constraints de seguridad**: el agente puede explorar y aprender, pero **nunca en formas que violen límites operacionales predefinidos**. Esta integración de aprendizaje y seguridad es esencial para el despliegue de agentes autónomos en entornos humanos.

#### 1.3.4 Adaptación dinámica ante perturbaciones del entorno

La **capacidad de adaptación** es la marca distintiva de los sistemas cibernéticos avanzados. Mientras que los controladores clásicos están diseñados para operar en un rango específico de condiciones, los **sistemas adaptativos modifican sus propios parámetros o estructura en respuesta a cambios detectados en el entorno**. El **control adaptativo de modelos de referencia (MRAC)** y el **control auto-sintonizable (self-tuning control)** son paradigmas clásicos donde el controlador contiene un **estimador de parámetros en línea** que actualiza continuamente el modelo utilizado para el diseño de control.

En el contexto de agentes inteligentes, la adaptación ocurre en **múltiples escalas**. La más rápida es la **adaptación paramétrica**: ajuste de ganancias o pesos en respuesta a errores de predicción. Una escala intermedia es la **adaptación estructural**: modificación de la arquitectura del modelo o del controlador cuando la estructura actual resulta inadecuada. La más lenta es la **adaptación metacognitiva**: cambio en los objetivos mismos o en las estrategias de exploración basándose en evaluación del progreso de aprendizaje. Esta **jerarquía de adaptaciones**, cada una con sus propios mecanismos de estabilización, es esencial para la **robustez a largo plazo**.

La conexión con la teoría de la información es profunda: la adaptación puede entenderse como un proceso de **minimización de la tasa de información mutua entre las predicciones del agente y las observaciones reales**, es decir, **minimización de la sorpresa**. La conexión con la decisión bayesiana es que la **adaptación óptima**, en el sentido de maximizar la utilidad esperada a largo plazo, típicamente requiere balancear la explotación del modelo actual con la exploración de alternativas que podrían ser superiores. El control cibernético proporciona los mecanismos mediante los cuales esta adaptación ocurre de manera **estable y predecible**, sin oscilaciones inestables o convergencia a mínimos locales subóptimos.

## 2. Marcos Teóricos de Unificación

La identificación de tres pilares fundamentales —Teoría de la Información, Decisión Bayesiana y Control Cibernético— como componentes esenciales de cualquier agente que aprende naturalmente conduce a la pregunta de si existen **marcos teóricos que los integren de manera principled**. Afortunadamente, varios desarrollos recientes en neurociencia computacional, ingeniería de control y aprendizaje automático han producido exactamente tales marcos unificadores. Estos no son meras curiosidades teóricas sino que tienen **implicaciones prácticas directas** para el diseño de algoritmos y arquitecturas de agentes.

### 2.1 Principio de Energía Libre (Free Energy Principle)

#### 2.1.1 Minimización de energía libre variacional como objetivo unificado

El **Principio de Energía Libre**, desarrollado por **Karl Friston** y colaboradores, representa quizás el intento más ambicioso de **unificación de los tres pilares bajo un principio variacional único**. La afirmación central es que **todo sistema que mantiene sus estados sensibles dentro de límites viables** (es decir, que "existe" en un sentido biológico) **puede interpretarse como minimizando una cantidad llamada "energía libre variacional"**, que es una **cota superior de la sorpresa (surprisal) de sus observaciones** .

La energía libre **F** se define formalmente como:

**F = E_q[log q(s) − log p(o,s)] = D_KL(q(s) || p(s|o)) − log p(o)**

donde **q(s)** es una aproximación al posterior sobre variables latentes **s**, **p(o,s)** es el **modelo generativo conjunto** de latentes y observaciones **o**, y la expectativa es bajo **q**. Esta cantidad puede reescribirse como la **divergencia KL entre la aproximación y el posterior verdadero**, menos la **log-evidencia (negativa de la sorpresa)**. Como la divergencia KL es siempre no-negativa, **F es una cota superior de la sorpresa**: minimizar F garantiza que la sorpresa no exceda este valor.

La relevancia para la unificación de nuestros tres pilares es inmediata. El **término de divergencia KL** conecta con la **Teoría de la Información**: minimizar esta divergencia es equivalente a hacer que las creencias internas del agente sean informativas sobre las causas de las observaciones. El **término de log-evidencia** conecta con la **Decisión Bayesiana**: maximizar la evidencia corresponde a seleccionar el modelo que mejor predice los datos, lo cual es la base del aprendizaje inductivo bayesiano. La **estructura de minimización iterativa** conecta con el **Control Cibernético**: el agente actúa para modificar sus observaciones de manera que sean más predecibles, es decir, para **mantenerse en estados de baja sorpresa**, lo cual es **homeostasis en términos informacionales** .

#### 2.1.2 Inferencia activa: percepción y acción como procesos inferenciales

Una de las contribuciones más profundas del Principio de Energía Libre es la noción de **"inferencia activa" (active inference)**, donde **la acción se conceptualiza como un proceso inferencial del mismo tipo que la percepción**. Mientras que la percepción ajusta las creencias internas para coincidir mejor con las observaciones, **la acción ajusta las observaciones para coincidir mejor con las creencias** (específicamente, con las predicciones o "priors empíricos" sobre observaciones deseables). Esta simetría es matemáticamente elegante: **ambos procesos minimizan energía libre, pero operan sobre variables diferentes** (latentes vs. observaciones) .

En la práctica, la inferencia activa implica que el agente **genera predicciones sobre observaciones futuras deseables** (basadas en priors sobre estados preferidos) y luego **ejecuta acciones que minimizan la discrepancia predicha** entre estas predicciones y las observaciones que anticipa recibir. Esto es distinto del control óptimo clásico, donde las acciones se seleccionan para maximizar una función de utilidad explícita. En inferencia activa, **no hay función de utilidad separada**: los "objetivos" están codificados en los **priors sobre observaciones**, y el comportamiento óptimo emerge naturalmente de la **minimización de energía libre** .

La **energía libre esperada (Expected Free Energy, EFE)** para una política π se descompone de manera reveladora :

**G(π) = −I(s;o|π)︸valor epistémico + E_q(o|π)[C(o)]︸valor pragmático**

o equivalentemente:

**G(π) = E_q(o,s|π)[log q(s|π) − log p̃(o,s)]**

donde **p̃(o,s) = p(o|s)p(s)** incorpora **preferencias** sobre observaciones. Esta descomposición muestra explícitamente cómo la política óptima **equilibra intrínsecamente la reducción de incertidumbre (exploración epistémica) con la consecución de objetivos (explotación pragmática)**. No se requiere ajuste de parámetros de exploración; **el equilibrio emerge naturalmente de la estructura del objetivo** .

#### 2.1.3 Equilibrio entre precisión sensorial y complejidad del modelo

La descomposición de la energía libre en términos de **precisión y complejidad** revela un **trade-off fundamental** en el diseño de agentes. La **precisión sensorial** —la capacidad del modelo para predecir observaciones— se maximiza con modelos complejos que capturen todas las regularidades del entorno. Sin embargo, la **complejidad del modelo** —medida típicamente como la divergencia KL entre posterior y prior— **penaliza modelos demasiado elaborados** que "sobreajusten" a fluctuaciones aleatorias. El principio de energía libre **implementa automáticamente la "navaja de Occam"**: dado igual ajuste a los datos, se prefieren modelos más simples .

Este trade-off tiene implicaciones prácticas para la arquitectura de agentes. Los sistemas con **recursos computacionales limitados** deben necesariamente operar con **modelos aproximados**, y la energía libre proporciona un criterio para optimizar esta aproximación: **no se trata de la mayor precisión posible, sino de la mejor precisión por unidad de complejidad**. Esto justifica técnicas como el **"dropout"** en redes neuronales (que puede interpretarse como inferencia variacional aproximada) y la **compresión de modelos**: ambas buscan mantener el desempeño predictivo mientras se reduce la complejidad efectiva .

En el contexto de la percepción, el **balance precisión-complejidad** explica fenómenos como la **"percepción como inferencia"** y las **ilusiones ópticas**. Cuando las observaciones son ambiguas, el sistema perceptual no reporta la interpretación más precisa en un sentido verídico, sino la interpretación que **minimiza energía libre dado el modelo interno**. Esto puede llevar a percepciones "incorrectas" si el prior es fuerte y la evidencia sensorial débil, pero estas percepciones son **óptimas en el sentido de que representan la mejor inferencia dada la información disponible y los costos de error** .

#### 2.1.4 Implementación con filtros de Kalman extendidos y descenso de gradiente

La minimización de energía libre puede implementarse mediante **diversos algoritmos**, dependiendo de la estructura del modelo generativo. Para **modelos lineales-gaussianos**, el **filtro de Kalman** proporciona la **solución exacta**: la actualización de creencias es precisamente la minimización de energía libre para este caso particular. Para **modelos no lineales**, el **filtro de Kalman extendido (EKF)** o el **filtro de partículas** proporcionan aproximaciones. El EKF linealiza localmente el modelo y aplica las ecuaciones de Kalman, mientras que los **filtros de partículas** representan el posterior como conjunto de muestras ponderadas .

Para **modelos con estructura de red neuronal**, el **descenso de gradiente estocástico** es el método de optimización estándar. La energía libre puede calcularse como una **función de pérdida diferenciable**, y sus gradientes con respecto a los parámetros del modelo pueden computarse mediante **backpropagation**. Esto ha permitido el desarrollo de **"modelos generativos profundos"** como VAEs, GANs y modelos de difusión, donde el entrenamiento corresponde a la minimización de alguna forma de energía libre (aunque típicamente con aproximaciones para hacer el cálculo tratable) .

La **inferencia activa** añade la complicación de que las acciones afectan las observaciones futuras, introduciendo **dependencias temporales en la optimización**. La solución estándar es la **planificación de horizonte deslizante**: en cada paso, el agente optimiza una secuencia de acciones futuras asumiendo que posteriormente volverá a optimizar, ejecuta la primera acción, observa, y repite. Esta aproximación recursiva es **computacionalmente demandante** pero puede acelerarse mediante métodos de **"inferencia amortizada"**, donde una red neuronal es entrenada para producir buenas acciones directamente .

### 2.2 Teoría de Control Estocástico e Información

#### 2.2.1 Control con restricciones de información (information-constrained control)

La **teoría de control con restricciones de información** estudia **sistemáticamente cómo las limitaciones en el flujo de información afectan el rendimiento alcanzable del control**. A diferencia del control clásico, que asume observaciones perfectas o ruido aditivo gaussiano, esta teoría considera **canales de comunicación con capacidad finita** entre sensores, controladores y actuadores. El resultado fundamental es que **existe un trade-off fundamental entre rendimiento de control y requerimientos de información**: para lograr una determinada regulación del error, se requiere una **tasa mínima de información** a través del canal de retroalimentación .

Este framework es directamente relevante para **agentes de IA con recursos limitados**. Un robot con **ancho de banda de comunicación restringido** entre sensores y unidad de procesamiento debe **comprimir selectivamente la información sensorial**, reteniendo solo aquella que es crítica para el control. La **cuantificación vectorial** y el **control predictivo basado en modelo con codificación de información** son técnicas que implementan explícitamente esta compresión. El diseño óptimo del sistema de control se convierte en un **problema conjunto de codificación de fuente** (qué transmitir) **y codificación de canal** (cómo transmitirlo) .

El **control con restricciones de tasa (rate-limited control)** ha demostrado que, para sistemas lineales, **la capacidad del canal de retroalimentación debe exceder la entropía de la planta** (medida de su "complejidad dinámica") para estabilización posible. Este resultado, que conecta la teoría de control con la teoría de la información de Shannon, **establece límites fundamentales que ningún algoritmo de aprendizaje puede superar**. Para el diseño de agentes, esto implica que la **arquitectura sensorial-procesamiento-actuación debe dimensionarse adecuadamente**: un sistema con sensores de alta resolución pero procesamiento limitado, o con actuadores sofisticados pero comunicación restringida, operará inherentemente por debajo de su potencial .

#### 2.2.2 Capacidad de canal y limitaciones fundamentales del control

La **capacidad de canal**, concepto central de la teoría de la información de Shannon, adquiere **interpretaciones operacionales profundas** en el contexto del control. Para un sistema de control en lazo cerrado, el "canal" incluye no solo los enlaces de comunicación explícitos sino también el **propio proceso de percepción-decisión-actuación**. La capacidad de este canal compuesto **limita qué tan rápidamente el sistema puede responder a perturbaciones**, qué tan precisamente puede regular variables internas, y qué tan robustamente puede mantener estabilidad .

Los **teoremas de codificación de canal** establecen que es posible transmitir información a tasas arbitrariamente cercanas a la capacidad con probabilidad de error arbitrariamente pequeña, dado suficiente complejidad de codificación. En el control, esto se traduce en que **el rendimiento óptimo es asintóticamente alcanzable** con esquemas de cuantificación y codificación suficientemente sofisticados. Sin embargo, la **complejidad de implementación** y los **retardos inducidos** pueden hacer que estos esquemas teóricamente óptimos sean imprácticos, motivando el desarrollo de **aproximaciones subóptimas pero computacionalmente eficientes** .

#### 2.2.3 Dualidad entre estimación de estado y control óptimo

La **dualidad entre estimación de estado óptima (filtrado de Kalman) y control óptimo (regulador lineal cuadrático, LQR)** es uno de los resultados más elegantes de la teoría de control moderna. Específicamente, **las ecuaciones de Riccati que determinan la ganancia de Kalman para la estimación óptima son matemáticamente idénticas a las ecuaciones que determinan la ganancia de retroalimentación óptima para el problema de control LQR**, con una transposición de matrices apropiada. Esta dualidad no es una coincidencia superficial, sino que **refleja una estructura profunda**: en ambos casos, el problema puede formularse como la **minimización de una forma cuadrática sujeta a restricciones dinámicas lineales** .

En el contexto de la Ciencia del Agente, esta dualidad sugiere que **los mecanismos de percepción y acción en un agente óptimo deben estar estrechamente acoplados**, con **estructuras matemáticas similares operando en direcciones complementarias**. La extensión de esta dualidad a **sistemas no lineales y no gaussianos** es un área activa de investigación, con conexiones a la **teoría de dualidad en optimización convexa** y a la **mecánica estadística de sistemas disipativos**. La formulación de información del problema de control revela que **la dualidad se extiende a los trade-offs información-rendimiento**: el costo de información en el canal de observación tiene un análogo directo en el costo de control en el actuador, y el diseño óptimo equilibra estos costos de manera simétrica .

#### 2.2.4 Aplicaciones en sistemas con recursos computacionales limitados

La relevancia práctica de la teoría de control con restricciones de información se manifiesta más claramente en **aplicaciones donde los recursos computacionales son severamente limitados**. Los **sistemas embebidos en robótica**, los **dispositivos de Internet de las Cosas**, y los **sensores autónomos de larga duración** operan típicamente con **microcontroladores de baja potencia, memoria limitada, y restricciones energéticas estrictas**. En estos contextos, **no es viable implementar algoritmos sofisticados de estimación y control** que asuman capacidad de procesamiento ilimitada .

La teoría de control con restricciones de información proporciona **principios de diseño** para crear sistemas que **aproximan el rendimiento óptimo dado los recursos disponibles**. Estrategias específicas incluyen: **cuantización adaptativa** de las observaciones, donde la resolución de los sensores se ajusta dinámicamente según la criticidad de la tarea; **esquemas de event-triggered control**, donde las actualizaciones de control solo ocurren cuando la información nueva es suficientemente significativa para justificar el costo energético de la comunicación; y **aproximaciones de modelos de orden reducido**, donde se mantiene explícitamente un **trade-off entre la complejidad del modelo y la precisión de la predicción**. Estas técnicas, aunque desarrolladas originalmente en ingeniería de control, son **directamente aplicables al diseño de agentes de IA con arquitecturas de procesamiento eficientes**, y conectan con los principios de máxima entropía y compresión de información discutidos anteriormente .

### 2.3 Aprendizaje por Refuerzo Bayesiano

#### 2.3.1 Procesos de decisión de Markov parcialmente observables (POMDPs)

Los **Procesos de Decisión de Markov Parcialmente Observables (POMDPs)** proporcionan el **marco matemático más general para la toma de decisiones secuencial bajo incertidumbre sobre el estado del mundo**. A diferencia de los MDPs completamente observables, donde el agente conoce el estado exacto en cada paso, en un POMDP el agente **solo recibe observaciones ruidosas o incompletas** que proporcionan información indirecta sobre el estado verdadero. Formalmente, un POMDP se define por la tupla **(S, A, O, T, Z, R, γ)**, donde **S** es el espacio de estados, **A** el espacio de acciones, **O** el espacio de observaciones, **T(s'|s,a)** las dinámicas de transición, **Z(o|s',a)** el modelo de observación, **R(s,a)** la función de recompensa, y **γ** el factor de descuento .

La **solución óptima de un POMDP** requiere mantener una **distribución de creencias b(s) = P(s|h)** sobre los estados, condicionada en toda la historia de interacción **h**, y seleccionar acciones basadas en esta creencia **rather than** en un estado puntual. La actualización de creencias sigue la regla de **filtrado bayesiano**:

**b'(s') ∝ Z(o|s',a) Σ_s T(s'|s,a) b(s)**

Esta recursión es **computacionalmente intratable** para espacios de estado grandes o continuos, motivando el desarrollo de **aproximaciones**. Los **métodos de punto de creencia (belief point methods)** mantienen un conjunto finito de creencias representativas; los **métodos de Monte Carlo** aproximan la expectativa sobre trayectorias mediante simulación; y los **métodos de función de valor aproximada** parametrizan la función de valor sobre creencias con arquitecturas de red neuronal .

#### 2.3.2 Métodos de Monte Carlo para inferencia aproximada

Los **métodos de Monte Carlo**, incluyendo **muestreo de importancia, cadenas de Markov (MCMC), y métodos de partículas**, proporcionan **herramientas flexibles para aproximar distribuciones de creencia y expectativas** en POMDPs. El **filtro de partículas (SMC, Sequential Monte Carlo)** mantiene una representación de la distribución de creencia como **conjunto de muestras ponderadas (partículas)**, actualizadas mediante re-muestreo cuando la degeneración de pesos lo requiere. Para problemas de alta dimensionalidad, el número de partículas necesario para una aproximación adecuada puede crecer exponencialmente, motivando **variantes que explotan estructura** (Rao-Blackwellized particle filters) o utilizan **dinámicas de proposición más sofisticadas** .

La integración de métodos de Monte Carlo con **optimización de políticas** ha producido algoritmos como **POMCP (Partially Observable Monte Carlo Planning)** y sus extensiones, que utilizan **simulaciones para construir árboles de búsqueda sobre historias de acción-observación**. Estos métodos han demostrado **desempeño sobresaliente en problemas de escala moderada**, incluyendo juegos de cartas parcialmente observables y navegación robótica. La **convergencia asintótica** de estos métodos está garantizada bajo condiciones de exploración adecuada, aunque las **tasas de convergencia pueden ser lentas** para problemas con horizontes largos o recompensas raras .

#### 2.3.3 Integración de priors bayesianos en algoritmos de RL

La **integración de conocimiento previo en algoritmos de aprendizaje por refuerzo**, mediante **priors bayesianos sobre dinámicas, recompensas, o políticas óptimas**, puede **acelerar dramáticamente el aprendizado y mejorar la generalización**. Los **priors sobre dinámicas** codifican conocimiento físico (leyes de movimiento, conservación de energía) o estructural (invariancias, jerarquías temporales). Los **priors sobre recompensas** pueden especificar objetivos parciales o restricciones de seguridad. Los **priors sobre políticas**, implementados mediante regularización o inicialización de redes de política, pueden **transferir conocimiento de tareas relacionadas** .

La **inferencia bayesiana completa sobre modelos de dinámica**, manteniendo un posterior **P(θ|D)** sobre parámetros del modelo dado datos observados, permite la **planificación con incertidumbre epistémica**: el agente considera no solo su mejor estimación de la dinámica, sino la **distribución sobre dinámicas posibles**. Esto produce **comportamiento de exploración sofisticado**, donde el agente busca activamente información en regiones del espacio de estado donde la incertidumbre sobre la dinámica impacta significativamente el valor de las acciones. Algoritmos como **PILCO (Probabilistic Inference and Learning for Control)** y sucesores implementan esta aproximación para sistemas con dinámicas continuas, utilizando **procesos gaussianos** para representar la incertidumbre sobre la función de transición .

#### 2.3.4 Muestreo de Thompson y sus extensiones informacionales

El **muestreo de Thompson**, ya introducido como mecanismo de balance exploración-explotación, puede **extenderse para considerar explícitamente el valor de la información en contextos secuenciales**. En su forma básica para bandits, el muestreo de Thompson ya implementa una forma de **exploración informada por incertidumbre**: la probabilidad de seleccionar una acción corresponde a su probabilidad de ser óptima, lo que **concentra automáticamente la exploración en regiones de incertidumbre relevante para la decisión** .

Las **extensiones informacionales** incorporan explícitamente consideraciones de **ganancia de información esperada** en la selección de acciones. El **Information-Directed Sampling (IDS)** y variantes relacionadas **ponderan la utilidad esperada por la información ganada**, produciendo algoritmos con **mejores garantías de regret en ciertos escenarios**. La conexión con el marco de inferencia activa es profunda: ambos enfoques **derivan el balance exploración-explotación de principios information-theoretic**, aunque con formulaciones matemáticas ligeramente diferentes .

## 3. Implementaciones y Simulaciones Concretas

La traducción de marcos teóricos unificados en **implementaciones computacionales funcionales** ha progresado significativamente en los últimos años, con varios proyectos destacados que demuestran la **viabilidad práctica** de la visión integradora de la Ciencia del Agente.

### 3.1 Free Energy Projective Simulation (FEPS)

#### 3.1.1 Arquitectura de memoria episódica con clips y transiciones

El modelo **Free Energy Projective Simulation (FEPS)**, desarrollado por Pazem, Krumm, Vining, Fiderer y Briegel, representa una **implementación concreta y novedosa que integra el Principio de Energía Libre con el marco de Projective Simulation**, ofreciendo un **agente de inferencia activa interpretable sin depender de redes neuronales profundas** . Esta arquitectura constituye un avance significativo porque aborda explícitamente uno de los problemas centrales en la implementación de agentes inteligentes: **la tensión entre expresividad del modelo e interpretabilidad de sus procesos de decisión**.

La arquitectura de memoria episódica con **clips y transiciones** en FEPS se organiza en dos componentes principales: el **modelo del mundo (world model)** y la **política de acción (policy)**. El modelo del mundo se implementa como un **grafo dirigido** donde los vértices representan **observaciones sensoriales** (cuadrados en la visualización del artículo) y los **clips clones** representan todos los valores posibles que un estado de creencia puede tomar (círculos). Cada clip clone se relaciona exactamente con una observación, y
